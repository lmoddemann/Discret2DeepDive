{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import plotly.graph_objects as go\n",
    "import yaml\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Optional\n",
    "import time\n",
    "from sympy import *\n",
    "import numpy as np\n",
    "\n",
    "from seqcat_datamodule import Dataset\n",
    "from seqcat_catvae import seqcat_vae\n",
    "from seq_vae import seq_vae\n",
    "from discret2dive_utils import load_model, assosciation_rule_mining, create_dict, save_files\n",
    "from preprocessing_rules import getHealthStates\n",
    "from diagnoser import Diag_solver\n",
    "from translate import parse_file\n",
    "\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diagnosis(anomaly):\n",
    "    # specify path where to find the diagnosis rules\n",
    "    rule_path = 'tank_rule_complete'\n",
    "    rules = parse_file(f'diagnosis/{rule_path}.txt', is_sympy=False)\n",
    "    # load trained model\n",
    "    MODEL_VERSION = f'VAE_training_hparams/tank_discret2deepdive'\n",
    "    ckpt_file_name = os.listdir(f'./{MODEL_VERSION}/checkpoints/')[-1]\n",
    "    ckpt_file_path = f'./{MODEL_VERSION}/checkpoints/{ckpt_file_name}'\n",
    "    with open(f'./{MODEL_VERSION}/hparams.yaml') as f:\n",
    "        hparam = yaml.safe_load(f)\n",
    "    model = seqcat_vae.load_from_checkpoint(ckpt_file_path, hparams=hparam[\"hparams\"])\n",
    "\n",
    "    MODEL_VERSION_SEQ = 'VAE_training_hparams/tank/seq_vae'\n",
    "    threshold = -170\n",
    "    ckpt_file_name_seq = os.listdir(f'./{MODEL_VERSION_SEQ}/checkpoints/')[-1]\n",
    "    ckpt_file_path_seq = f'./{MODEL_VERSION_SEQ}/checkpoints/{ckpt_file_name_seq}'\n",
    "    with open(f'./{MODEL_VERSION_SEQ}/hparams.yaml') as f:\n",
    "        hparam_seq = yaml.safe_load(f)\n",
    "    model_seq = seq_vae.load_from_checkpoint(checkpoint_path=ckpt_file_path_seq, hparams = hparam_seq)\n",
    "    \n",
    "    # read normal data\n",
    "    if anomaly=='norm':\n",
    "        df_csv = pd.read_csv(f'preprocessed_data/tank_simulation/norm_long.csv').iloc[:, :3].reset_index(drop=True)\n",
    "        df_csv_realcat = pd.read_csv(f'preprocessed_data/tank_simulation/norm_long.csv').reset_index(drop=True).iloc[10:2000, 3].reset_index(drop=True)\n",
    "        scaler = StandardScaler().fit(pd.read_csv('preprocessed_data/tank_simulation/norm_long.csv').iloc[:, :3].reset_index(drop=True))\n",
    "        df_csv_sc = pd.DataFrame(scaler.transform(df_csv), columns=df_csv.columns, index=df_csv.index).iloc[:2000, :].reset_index(drop=True)\n",
    "\n",
    "    # read anomalous data\n",
    "    else:\n",
    "        df_csv = pd.read_csv(f'preprocessed_data/tank_simulation/{anomaly}_long_faulty.csv').reset_index(drop=True).iloc[:, :3]\n",
    "        df_csv_realcat = pd.read_csv(f'preprocessed_data/tank_simulation/{anomaly}_long_faulty.csv').iloc[1510:2500, 3].reset_index(drop=True)\n",
    "        scaler = StandardScaler().fit(pd.read_csv('preprocessed_data/tank_simulation/norm.csv').iloc[1000:, :3].reset_index(drop=True))\n",
    "        df_csv_sc = pd.DataFrame(scaler.transform(df_csv), columns=df_csv.columns, index=df_csv.index).iloc[1500:2500, :].reset_index(drop=True)\n",
    "    faulty_idx = df_csv_realcat.str.contains('faulty').astype(int)\n",
    "    dataset = Dataset(dataframe=df_csv_sc.iloc[:, :3].reset_index(drop=True), number_timesteps=hparam[\"hparams\"][\"NUMBER_TIMESTEPS\"])\n",
    "\n",
    "    #load automaton\n",
    "    with open('preprocessed_data/automaton.json', 'r') as json_file:\n",
    "        automaton = json.load(json_file)\n",
    "    # compute initial discretization\n",
    "    pzx_logits, pzx, mu, sigma, pxz, z = model.get_states(dataset[0].unsqueeze(0).to('cuda'))\n",
    "    prev_cat = z.argmax().detach().cpu().numpy().astype(int)\n",
    "\n",
    "    all_cats = []\n",
    "    all_kl = []\n",
    "    all_like = []\n",
    "    all_residuals = []\n",
    "    all_mu = []\n",
    "    diag = []\n",
    "    diag_true = []\n",
    "    cat_diag = []\n",
    "    transistion_valid = True\n",
    "    for i in range(1, len(dataset)-1):\n",
    "        window = dataset[i]\n",
    "        predicted, x = model_seq(window.unsqueeze(0).to('cuda'))  \n",
    "        residuals = np.abs(window - predicted.detach().cpu())  # Calculate absolute residuals\n",
    "        all_residuals.extend(x['recon_loss'].flatten().detach().cpu().numpy())\n",
    "        pzx_logits, pzx, mu, sigma, pxz, z = model.get_states(window.unsqueeze(0).to('cuda'))\n",
    "        _, kl = model.kl_divergence(pzx=pzx)\n",
    "\n",
    "        # computing actual category\n",
    "        actual_cat = z.argmax().detach().cpu().numpy().astype(int)\n",
    "        like = model.function_likelihood(x=window.unsqueeze(0).to('cuda')).mean()\n",
    "\n",
    "        # check for a transition happened\n",
    "        if prev_cat.item() != actual_cat.item(): \n",
    "            # check transistion is ok or not ok\n",
    "            valid_transitions = automaton.get(str(prev_cat.item()), {})\n",
    "            if str(actual_cat) in valid_transitions:\n",
    "                transistion_valid = True\n",
    "            else:\n",
    "                transistion_valid = False\n",
    "\n",
    "        # check on anomaly in catvae and seq_vae\n",
    "        threshold_catvae_like = -8\n",
    "        threshold_seqvae_res = -160\n",
    "        seqvae_res_anom = x['recon_loss'].flatten().detach().cpu().numpy() < threshold_seqvae_res\n",
    "        catvae_res_anom = like.flatten().detach().cpu().numpy() < threshold_catvae_like\n",
    "\n",
    "        # check the combination of wrong transitions and anomalies\n",
    "        if seqvae_res_anom[0] and transistion_valid==True: \n",
    "            one_hot_encoded = np.zeros(12, dtype=int)\n",
    "            one_hot_encoded[actual_cat] = 1\n",
    "            mode_anom = ''.join('b' if x == 1 else 'a' for x in one_hot_encoded)\n",
    "            diag.append(mode_anom)\n",
    "            cat_diag.append(actual_cat)\n",
    "            prev_cat = z.argmax().detach().cpu().numpy().astype(int)\n",
    "\n",
    "        elif transistion_valid==False:\n",
    "            one_hot_encoded = np.zeros(12, dtype=int)\n",
    "            cat = int(list(automaton.get(str(prev_cat), None).keys())[0])\n",
    "            one_hot_encoded[cat] = 1\n",
    "            mode_anom = ''.join('b' if x == 1 else 'a' for x in one_hot_encoded)\n",
    "            diag.append(mode_anom)\n",
    "            cat_diag.append(int(list(automaton.get(str(prev_cat), None).keys())[0]))\n",
    "\n",
    "        else: \n",
    "            diag.append(-1)\n",
    "            cat_diag.append(-1)\n",
    "            prev_cat = z.argmax().detach().cpu().numpy().astype(int)\n",
    "\n",
    "        z_list = z.detach().cpu().numpy().astype(int) \n",
    "        all_cats.append(z_list)\n",
    "        all_kl.append(kl.detach().cpu().numpy())\n",
    "        all_like.append(like.detach().cpu().numpy())\n",
    "        all_mu.append(mu.detach().cpu().numpy())\n",
    "\n",
    "        # making diagnosis \n",
    "        filtered_diag = pd.DataFrame(diag)[pd.DataFrame(diag)[0] != -1]\n",
    "        # Check if the filtered DataFrame is empty\n",
    "        if len(filtered_diag) == 0:\n",
    "            faultStates = []  # or np.array([]) if you prefer a numpy array\n",
    "            diag_true.append({'Index': i, 'Diag_Correct': 0})\n",
    "\n",
    "        else:\n",
    "            # Determine the number of rows to slice (either 100 or the length of filtered_diag)\n",
    "            last_100_faults = min(100, len(filtered_diag))\n",
    "            # Get the unique values from the last 100 (or fewer) rows of column 0\n",
    "            faultStates = filtered_diag[-last_100_faults:][0].unique()\n",
    "\n",
    "            fault_States = {key: False for key in faultStates}\n",
    "            rules_healthStates = getHealthStates(rules=rules, faultStates=fault_States)\n",
    "            diag_model = Diag_solver(rules=rules, health_dict=rules_healthStates)\n",
    "            diag_diagnosis, min_causes, causes, delta_time = diag_model.solve()\n",
    "            # check whether diagnosis is ok (solution is the diagnosis) or not ok. Will be plotted within the plot.\n",
    "            checkDiag = any(anomaly.split('_')[0] == str(item) for sublist in diag_model.solve()[0] for item in sublist)\n",
    "            if checkDiag == True:\n",
    "                diag_true.append({'Index': i, 'Diag_Correct': 1})\n",
    "            else: \n",
    "                diag_true.append({'Index': i, 'Diag_Correct': -1})\n",
    "\n",
    "\n",
    "    all_residuals = np.array(all_residuals)\n",
    "    anom_labels = np.where(all_residuals < threshold, .5 , 0)\n",
    "\n",
    "    all = pd.DataFrame(np.vstack(all_cats))\n",
    "    kl_ = pd.DataFrame(np.vstack(all_kl))\n",
    "    cats = pd.DataFrame(all.idxmax(axis=1))\n",
    "    like_ = pd.DataFrame(all_like)\n",
    "    mu_ = pd.DataFrame(np.vstack(all_mu)[::10].reshape(-1, np.vstack(all_mu)[::10].shape[2]))\n",
    "    data_ = pd.DataFrame(np.vstack(dataset)[::10].reshape(-1, np.vstack(all_mu)[::10].shape[2]))\n",
    "    unique_cats = cats[cats.columns[0]].unique()\n",
    "\n",
    "    fig = make_subplots(rows=7, cols=1, shared_xaxes=True)\n",
    "    for i in range(0,3):\n",
    "        fig.add_trace(go.Scatter(x=pd.DataFrame(df_csv_sc).index, y=pd.DataFrame(df_csv_sc)[pd.DataFrame(df_csv_sc).columns[i]], name=df_csv_sc.columns[i],  mode='markers'), \n",
    "                    row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x = pd.DataFrame(df_csv_sc).index, y=cats[cats.columns[0]], name='discretized category', mode='lines'),row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(x=pd.DataFrame(df_csv_realcat).index, y=df_csv_realcat.values, name='real category', mode='markers'), row=3, col=1)\n",
    "    fig.add_trace(go.Scatter(x=pd.DataFrame(df_csv_sc).index, y=all_residuals, name='residual', mode='markers'), row=4, col=1)\n",
    "    fig.add_trace(go.Scatter(x=df_csv.index, y=like_[like_.columns[0]], mode='lines', name='likelihood'), row=5, col=1) # was row=5\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=pd.DataFrame(df_csv_sc).index, y=faulty_idx, mode='lines', name='induced anomaly'), row=6, col=1)\n",
    "    fig.add_trace(go.Scatter(x=pd.DataFrame(df_csv_sc).index, y=anom_labels, mode='lines', name='anomaly indicator'), row=6, col=1)\n",
    "    fig.add_trace(go.Scatter(x=pd.DataFrame(df_csv_sc).index, y=cat_diag, mode='lines', name='diagosed category'), row=7, col=1)\n",
    "    fig.add_trace(go.Scatter(x=pd.DataFrame(df_csv_sc).index, y=pd.DataFrame(diag_true).iloc[:,1], mode='lines', name='true diagnosis? 1-ok, -1-notok'), row=7, col=1)\n",
    "\n",
    "    fig.update_layout(title_text=anomaly)\n",
    "    fig.show()\n",
    "    return diag_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_true = interact(plot_diagnosis, anomaly=['v3_50s','q1_50s', 'v12_50s', 'v23_50s', \n",
    "                             'q1short1s', 'v12short1s', 'v23short1s', 'v3short1s',\n",
    "                               'q1_100s', 'v12_100s', 'v23_100s', 'v3_100s', 'norm'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "improved_discret2di",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
